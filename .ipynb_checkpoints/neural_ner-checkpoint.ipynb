{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use a recurrent neural network to solve Named Entity Recognition (NER) problem. NER is a common task in natural language processing systems. It serves for extraction such entities from the text as persons, organizations, locations, etc. We will build a NER to recognize named entities from Twitter.\n",
    "\n",
    "For example, we want to extract persons' and organizations' names from the text. Than for the input text:\n",
    "\n",
    "    Ian Goodfellow works for Google Brain\n",
    "\n",
    "a NER model needs to provide the following sequence of tags:\n",
    "\n",
    "    B-PER I-PER    O     O   B-ORG  I-ORG\n",
    "\n",
    "Where *B-* and *I-* prefixes stand for the beginning and inside of the entity, while *O* stands for out of tag or no tag. Markup with the prefix scheme is called *BIO markup*. This markup is introduced for distinguishing of consequent entities with similar types.\n",
    "\n",
    "A solution of the task will be based on neural networks, particularly, on Bi-Directional Long Short-Term Memory Networks (Bi-LSTMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time \n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "data_path = \"./data/twitter\"\n",
    "path_to_logdir = './logdir'\n",
    "path_to_model = \"./models\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if not os.path.exists(path_to_logdir):\n",
    "    os.makedirs(path_to_logdir)\n",
    "    \n",
    "if not os.path.exists(path_to_model):\n",
    "    os.makedirs(path_to_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/- Load the Twitter Named Entity Recognition corpus\n",
    "\n",
    "We will work with a corpus, which contains tweets with NE tags. Every line of a file contains a pair of a token (word/punctuation symbol) and a tag, separated by a whitespace. Different tweets are separated by an empty line.\n",
    "\n",
    "## 1) Read data\n",
    "The function *read_data* reads a corpus from the *file_path* and returns two lists: one with tokens and one with the corresponding tags. You need to complete this function by adding a code, which will replace a user's nickname to `<USR>` token and any URL to `<URL>` token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    \n",
    "    tweet_tokens = []\n",
    "    tweet_tags = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if tweet_tokens:\n",
    "                tokens.append(tweet_tokens)\n",
    "                tags.append(tweet_tags)\n",
    "            tweet_tokens = []\n",
    "            tweet_tags = []\n",
    "        else:\n",
    "            token, tag = line.split()\n",
    "\n",
    "            if token.startswith('http://') or token.startswith(\"https://\"):\n",
    "                token = \"<URL>\"\n",
    "            if token.startswith(\"@\"):\n",
    "                token = \"<USR>\"\n",
    "            \n",
    "            tweet_tokens.append(token)\n",
    "            tweet_tags.append(tag)\n",
    "            \n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can load three separate parts of the dataset:\n",
    " - *train* data for training the model;\n",
    " - *validation* data for evaluation and hyperparameters tuning;\n",
    " - *test* data for final evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_tags = read_data(os.path.join(data_path, 'train.txt'))\n",
    "validation_tokens, validation_tags = read_data(os.path.join(data_path, 'validation.txt'))\n",
    "test_tokens, test_tags = read_data(os.path.join(data_path, 'test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Prepare dictionnaries\n",
    "\n",
    "To train a neural network, we will use two mappings: \n",
    "- {token}$\\to${token id}: address the row in embeddings matrix for the current token;\n",
    "- {tag}$\\to${tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(tokens_or_tags, special_tokens):\n",
    "    \"\"\"\n",
    "        tokens_or_tags: a list of lists of tokens or tags\n",
    "        special_tokens: some special tokens\n",
    "    \"\"\"\n",
    "    tok2idx = defaultdict(lambda: 0)\n",
    "    idx2tok = {}\n",
    "    \n",
    "    index = 0\n",
    "    for special_token in special_tokens:\n",
    "        tok2idx[special_token] = index\n",
    "        idx2tok[index] = special_token\n",
    "        index += 1\n",
    "    \n",
    "   \n",
    "    for seq in tokens_or_tags:\n",
    "        for tok in seq:\n",
    "            if not tok in tok2idx:\n",
    "                tok2idx[tok] = index\n",
    "                idx2tok[index] = tok\n",
    "                index += 1\n",
    "    \n",
    "    return tok2idx, idx2tok\n",
    "\n",
    "\n",
    "# create the mapping between tokens and ids for a sentence\n",
    "def words2idxs(tokens_list):\n",
    "    return [token2idx[word] for word in tokens_list]\n",
    "\n",
    "def tags2idxs(tags_list):\n",
    "    return [tag2idx[tag] for tag in tags_list]\n",
    "\n",
    "def idxs2words(idxs):\n",
    "    return [idx2token[idx] for idx in idxs]\n",
    "\n",
    "def idxs2tags(idxs):\n",
    "    return [idx2tag[idx] for idx in idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the function *build_dict* we can make dictionaries for tokens and tags. Special tokens in our case will be:\n",
    " - `<UNK>` token for out of vocabulary tokens;\n",
    " - `<PAD>` token for padding sentence to the same length when we create batches of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<UNK>', '<PAD>']\n",
    "special_tags = ['O']\n",
    "\n",
    "# Create dictionaries \n",
    "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
    "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Create dataset and datalaoder\n",
    "\n",
    "We will creatr know dataset object and dataloader, that well enable us to load batches of tweets. The tricky part is that all sequences within a batch need to have the same length. So we will pad them with a special `<PAD>` token.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches_generator(batch_size, tokens, tags,\n",
    "                      shuffle=True, allow_smaller_last_batch=True):\n",
    "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
    "    \n",
    "    n_samples = len(tokens)\n",
    "    if shuffle:\n",
    "        order = np.random.permutation(n_samples)\n",
    "    else:\n",
    "        order = np.arange(n_samples)\n",
    "\n",
    "    n_batches = n_samples // batch_size\n",
    "    if allow_smaller_last_batch and n_samples % batch_size:\n",
    "        n_batches += 1\n",
    "\n",
    "    for k in range(n_batches):\n",
    "        batch_start = k * batch_size\n",
    "        batch_end = min((k + 1) * batch_size, n_samples)\n",
    "        current_batch_size = batch_end - batch_start\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        max_len_token = 0\n",
    "        for idx in order[batch_start: batch_end]:\n",
    "            x_list.append(words2idxs(tokens[idx]))\n",
    "            y_list.append(tags2idxs(tags[idx]))\n",
    "            max_len_token = max(max_len_token, len(tags[idx]))\n",
    "            \n",
    "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
    "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>']\n",
    "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
    "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
    "        for n in range(current_batch_size):\n",
    "            utt_len = len(x_list[n])\n",
    "            x[n, :utt_len] = x_list[n]\n",
    "            lengths[n] = utt_len\n",
    "            y[n, :utt_len] = y_list[n]\n",
    "        yield x, y, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II/- Create and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we create the model, i.e. class **NERTagger**, our model is a simple one it has only tree ayers, an embedding layer that transforms the indexe into vectors, the second layer is an LSTM layer and the last layer is a Linear layer that maps the output of the LSTM layer to the required dimension, i.e. the number of all possible tags. All those layers are implemented in pytorch and can be used easily. We define also the **train_one_epoch** function, that takes into charge the training procedure, it consists in loading batches, computing the output of the batch, coumputing by comparing the predicted output with the true target, the gradients of the loss function are then computed using *.backward()*, and finally a gradient descent step is done using *.step()*, we also provide **test_model** function that take care of evaluating the model on the validation/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, n_tags):\n",
    "        super(NERTagger, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_tags = n_tags\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, batch_first=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(self.hidden_dim,  self.n_tags)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=2)\n",
    "        return tag_scores\n",
    "    \n",
    "    \n",
    "    \n",
    "def train_one_epoch(model, train_tokens, train_tags, batch_size, criterion, optimizer, writer, epoch):\n",
    "    n_samples = len(train_tokens)\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    running_loss = 0\n",
    "    for batch_id, (x_batch, y_batch, lengths) in enumerate(batches_generator(batch_size, train_tokens, train_tags)):\n",
    "        x_batch = torch.tensor(x_batch).long().to(device)\n",
    "        y_batch = torch.tensor(y_batch).long().to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        outputs = ner_tagger(x_batch).permute(0, 2, 1)\n",
    "    \n",
    "        loss = criterion(outputs, y_batch)\n",
    "    \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "        y_pred = outputs.data.max(1)[1]\n",
    "    \n",
    "        correct += y_pred.eq(y_batch.data).cpu().sum() / y_batch.shape[1]\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "        if batch_id % 100 == 99:\n",
    "            writer.add_scalar('training loss',\n",
    "                              running_loss / 100,\n",
    "                              epoch * (n_samples // batch_size) + batch_id)\n",
    "\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_id * x_batch.shape[0], n_samples,\n",
    "                       100. * batch_id / (n_samples // batch_size), running_loss / 100))\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            writer.flush()\n",
    "\n",
    "    writer.add_scalar('Train/Loss', loss.item(), epoch)\n",
    "    writer.flush()\n",
    "    \n",
    "    \n",
    "    \n",
    "def test_model(model, tokens, tags, criterion, writer=None, device=None, epoch=None):\n",
    "    model.eval()\n",
    "    i, loss, correct, n = [0, 0, 0, 0]\n",
    "    n_samples = 0\n",
    "\n",
    "    print(\"Testing..\")\n",
    "    with torch.no_grad():\n",
    "        for batch_id, (x_batch, y_batch, lengths) in enumerate(batches_generator(batch_size, tokens, tags)):\n",
    "            x_batch = torch.tensor(x_batch).long().to(device)\n",
    "            y_batch = torch.tensor(y_batch).long().to(device)\n",
    "\n",
    "            outputs = model(x_batch).permute(0, 2, 1)\n",
    "\n",
    "            loss += criterion(outputs, y_batch)\n",
    "\n",
    "            y_pred = outputs.data.max(1)[1]\n",
    "            correct += y_pred.eq(y_batch.data).cpu().sum() / y_batch.shape[1]\n",
    "            n += 1\n",
    "            n_samples += int(x_batch.shape[0])\n",
    "\n",
    "    loss /= n  # loss function already averages over batch size\n",
    "    accuracy = 100. * correct / (n_samples)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, correct, n_samples,\n",
    "        accuracy))\n",
    "\n",
    "    if writer:\n",
    "        # Record loss and accuracy into the writer\n",
    "        writer.add_scalar('Test/Loss', loss, epoch)\n",
    "        writer.add_scalar('Test/Accuracy', accuracy, epoch)\n",
    "        writer.flush()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train_model(model, train_tokens, train_tags, validation_tokens, validation_tags, batch_size, criterion, optimizer, writer, n_epochs=10):\n",
    "    best_acc = 0.\n",
    "    for epoch in range(0, n_epochs):\n",
    "        print(\"Epoch %d\" % epoch)\n",
    "        train_one_epoch(model, train_tokens, train_tags, batch_size, criterion, optimizer, writer, epoch)\n",
    "        acc = test_model(model, validation_tokens, validation_tags, criterion, writer, device, epoch)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model, os.path.join(path_to_model, \"ner_best.pth\"))\n",
    "\n",
    "        writer.close()\n",
    "        \n",
    "\n",
    "def predict_sentence(sentence, tags=None, verbosity=0):\n",
    "    X = torch.tensor(words2idxs(sentence)).to(device).unsqueeze(0)\n",
    "    \n",
    "    y_pred = ner_tagger(X)\n",
    "    y_pred = y_pred.squeeze().max(1)[1].cpu()\n",
    "    \n",
    "    res = list(y_pred.numpy())\n",
    "    res = idxs2tags(res)\n",
    "\n",
    "    if tags:\n",
    "        y_true = torch.tensor(tags2idxs(true_tags))\n",
    "        correct = y_pred.eq(y_true.data).cpu().sum()\n",
    "        if verbosity:\n",
    "            print(\"Correct tags {}/{}\".format(int(correct), len(res)))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train now our model, note that tensorboard can be used to view the evolution of the training processus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_to_log_dir = './logdir'\n",
    "if not os.path.exists(PATH_to_log_dir):\n",
    "    os.makedirs(PATH_to_log_dir)\n",
    "    \n",
    "timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "writer = SummaryWriter(os.path.join(path_to_logdir, timestr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using tensorboard is easy, and can be done by running the following command on terminal\n",
    "\n",
    "    tensorboard --logdir {PATH_to_log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train Epoch: 0 [3168/5795 (55%)]\tLoss: 0.489507\n",
      "Testing..\n",
      "\n",
      "Test set: Average loss: 0.2572, Accuracy: 680/724 (93%)\n",
      "\n",
      "Epoch 1\n",
      "Train Epoch: 1 [3168/5795 (55%)]\tLoss: 0.251141\n",
      "Testing..\n",
      "\n",
      "Test set: Average loss: 0.2190, Accuracy: 682/724 (94%)\n",
      "\n",
      "Epoch 2\n",
      "Train Epoch: 2 [3168/5795 (55%)]\tLoss: 0.204384\n",
      "Testing..\n",
      "\n",
      "Test set: Average loss: 0.1920, Accuracy: 683/724 (94%)\n",
      "\n",
      "Epoch 3\n",
      "Train Epoch: 3 [3168/5795 (55%)]\tLoss: 0.171639\n",
      "Testing..\n",
      "\n",
      "Test set: Average loss: 0.1827, Accuracy: 687/724 (94%)\n",
      "\n",
      "Epoch 4\n",
      "Train Epoch: 4 [3168/5795 (55%)]\tLoss: 0.138704\n",
      "Testing..\n",
      "\n",
      "Test set: Average loss: 0.1792, Accuracy: 687/724 (94%)\n",
      "\n",
      "Epoch 5\n",
      "Train Epoch: 5 [3168/5795 (55%)]\tLoss: 0.118273\n",
      "Testing..\n",
      "\n",
      "Test set: Average loss: 0.1794, Accuracy: 686/724 (94%)\n",
      "\n",
      "Epoch 6\n",
      "Train Epoch: 6 [3168/5795 (55%)]\tLoss: 0.090431\n",
      "Testing..\n",
      "\n",
      "Test set: Average loss: 0.1894, Accuracy: 685/724 (94%)\n",
      "\n",
      "Epoch 7\n",
      "Train Epoch: 7 [3168/5795 (55%)]\tLoss: 0.069209\n",
      "Testing..\n",
      "\n",
      "Test set: Average loss: 0.2019, Accuracy: 687/724 (94%)\n",
      "\n",
      "Epoch 8\n",
      "Train Epoch: 8 [3168/5795 (55%)]\tLoss: 0.051343\n",
      "Testing..\n",
      "\n",
      "Test set: Average loss: 0.2098, Accuracy: 684/724 (94%)\n",
      "\n",
      "Epoch 9\n",
      "Train Epoch: 9 [3168/5795 (55%)]\tLoss: 0.035532\n",
      "Testing..\n",
      "\n",
      "Test set: Average loss: 0.2339, Accuracy: 682/724 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n_epochs = 10\n",
    "learning_rate = 0.005\n",
    "learning_rate_decay = 2\n",
    "dropout_keep_probability = 0.5\n",
    "hidden_dim = 512\n",
    "embedding_dim = 128\n",
    "\n",
    "\n",
    "ner_tagger = NERTagger(embedding_dim=embedding_dim, hidden_dim=hidden_dim,\n",
    "                       vocab_size=len(token2idx), n_tags=len(tag2idx))\n",
    "\n",
    "ner_tagger = ner_tagger.to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(ner_tagger.parameters())\n",
    "\n",
    "\n",
    "\n",
    "train_model(ner_tagger, train_tokens, train_tags, validation_tokens,\n",
    "            validation_tags, batch_size, criterion, optimizer,\n",
    "            writer, n_epochs=n_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III/- Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we give the performence of the Neural NER tagger on the test set. Furthermore, we give some examples of the application of the trained on sentences from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing..\n",
      "\n",
      "Test set: Average loss: 0.2280, Accuracy: 676/724 (93%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc = test_model(ner_tagger, test_tokens, test_tags, criterion, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that our model gives good results, with approximately 95% of coorect tags. This result can even be improved by using more complex architectures and by spending more time on fine-tuining the hyperparameters. However accuracy is not enough to understand the quality of the model, because of the presence of a lot of 'O' tags in comparaison with other other tags. We also use F1 score and Precision and Recall metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_test = []\n",
    "for ii, sent in enumerate(test_tokens):\n",
    "    if len(sent) > 1:\n",
    "        y_pred.append(predict_sentence(sent))\n",
    "        y_test.append(test_tags[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(tag2idx)\n",
    "labels.remove('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3486622946970767"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "    B-company      0.707     0.345     0.464        84\n",
      "    I-company      0.611     0.275     0.379        40\n",
      "   B-facility      0.537     0.468     0.500        47\n",
      "   I-facility      0.765     0.426     0.547        61\n",
      "    B-geo-loc      0.631     0.497     0.556       165\n",
      "    I-geo-loc      0.760     0.365     0.494        52\n",
      "      B-movie      0.000     0.000     0.000         8\n",
      "      I-movie      0.333     0.100     0.154        10\n",
      "B-musicartist      0.167     0.111     0.133        27\n",
      "I-musicartist      0.000     0.000     0.000        24\n",
      "      B-other      0.418     0.320     0.363       103\n",
      "      I-other      0.277     0.247     0.261        93\n",
      "     B-person      0.103     0.471     0.169       104\n",
      "     I-person      0.340     0.258     0.293        66\n",
      "    B-product      0.500     0.107     0.176        28\n",
      "    I-product      0.349     0.250     0.291        60\n",
      " B-sportsteam      0.250     0.032     0.057        31\n",
      " I-sportsteam      0.500     0.083     0.143        12\n",
      "     B-tvshow      0.000     0.000     0.000         7\n",
      "     I-tvshow      0.000     0.000     0.000         5\n",
      "\n",
      "    micro avg      0.316     0.326     0.321      1027\n",
      "    macro avg      0.362     0.218     0.249      1027\n",
      " weighted avg      0.446     0.326     0.349      1027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give now some examples of usage of the traned NER tagger on different examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We consider\n",
      "Input sentence:  Prayers going out to the victims and families of the Wilmington Courthouse shooting .\n",
      "Correct tags 12/14\n",
      "Predicted NER tag O O O O O O O O O O B-other I-other O O\n",
      "****************************************\n",
      "We consider\n",
      "Input sentence:  Prayers going out to the victims of the San Bernardino shooting , can't understand what drives a group of people to do something so vile .\n",
      "Correct tags 26/26\n",
      "Predicted NER tag O O O O O O O O B-geo-loc I-geo-loc O O O O O O O O O O O O O O O O\n",
      "****************************************\n",
      "We consider\n",
      "Input sentence:  x1x_ne_x1x <URL> August 11 , 2015 at 02:02 AM 4\n",
      "Correct tags 10/10\n",
      "Predicted NER tag O O O O O O O O O O\n",
      "****************************************\n",
      "We consider\n",
      "Input sentence:  havoc sia flight tomorrow at 1145am and i havent even packed\n",
      "Correct tags 7/11\n",
      "Predicted NER tag B-person B-person O O O B-facility I-facility O O O O\n",
      "****************************************\n",
      "We consider\n",
      "Input sentence:  RT <USR> : There will be no classes for Iowa City Schools on Monday , February 2nd . <URL>\n",
      "Correct tags 16/19\n",
      "Predicted NER tag O O O O O O O O O B-person I-person O O O O O O O O\n",
      "****************************************\n",
      "We consider\n",
      "Input sentence:  kahit may fallback na ako , ust padin wewzs\n",
      "Correct tags 9/9\n",
      "Predicted NER tag O O O O O O O O O\n",
      "****************************************\n",
      "We consider\n",
      "Input sentence:  I just had to step into my office ( front porch ) again . Lol . I feel like everything happening today is highly unusual . I like it a lot !\n",
      "Correct tags 32/32\n",
      "Predicted NER tag O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "****************************************\n",
      "We consider\n",
      "Input sentence:  FBI probing major #security breach at Juniper Networks stock drops - #infosec <URL> … via <USR>\n",
      "Correct tags 15/16\n",
      "Predicted NER tag B-other O O O O O B-company I-company I-facility O O O O O O O\n",
      "****************************************\n",
      "We consider\n",
      "Input sentence:  Flash Flood Warning issued May 24 at 4:00 PM MDT until May 25 at 12:00 AM MDT by NWS <URL> #WxWY\n",
      "Correct tags 20/21\n",
      "Predicted NER tag O O O O O O O O O O O O O O O O O O O O O\n",
      "****************************************\n",
      "We consider\n",
      "Input sentence:  When the last time you ran into Rick Ross and Drake twice in the same day at 2 diff video shoot locations . Today I did !\n",
      "Correct tags 24/27\n",
      "Predicted NER tag O O O O O O O O B-person O O O O O O O O O O O O O O O O O O\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "ner_tagger.eval()\n",
    "\n",
    "\n",
    "indexes = random.sample(range(1, len(test_tokens)), 10)\n",
    "\n",
    "for idx in indexes:\n",
    "    sentence = test_tokens[idx]\n",
    "    true_tags = test_tags[idx]\n",
    "    print(\"We consider\")\n",
    "    print(\"Input sentence: \", ' '.join(sentence))\n",
    "    print(\"Predicted NER tag\", ' '.join(predict_sentence(sentence, tags=true_tags, verbosity=1)))\n",
    "    print(\"*\"*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
